{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Are Haters Just Hating? A Statistical Analysis of CSGO Pro Gamers\n",
    "<center> Sam Felsenfeld, Cindy Jia, Mark Levin </center>\n",
    "\n",
    "\n",
    "#\n",
    "### <center> What is Counterstrike: Global Offensive?</center>\n",
    "Counterstrike: Global Offensive - or CS:GO for short - is a multiplayer first-person shooter game with millions of active players. In a game of CS:GO, players compete to kill the opposing team in a virtual shootout while trying to complete objectives like planting or defusing a bomb. While the game itself is fairly competitive, it also boasts a large professional scene where the top players compete in five person teams to be the best in the world. Major tournaments take place almost every week, leading to a constant trove of player data and game statistics.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Player skill\n",
    "While there is often a clear consensus of which teams are the best during a given time period, the individual skill of players within these teams is often hard to connect to actual team performance: for example a team which has the consensus best player in the world at the time, Vitality, is currently ranked second behind Heroic, a team which has only a single player in the top 20 players of 2022. In a game like CS:GO, an individual's performance is often the deciding factor between winning and losing a match. When star players have slumps, there often seems to be a clear impact on team performance, leading to much criticism from the public.\n",
    "\n",
    "\n",
    "\n",
    "Public opinion can often be a misrepresentation of the actual performance of public figures. In this analysis, we aim to determine whether the public perception that a player is either at the top of their game or on their way out is actually reality, or just something that people like to overemphasize.\n",
    "\n",
    "## <center> Data Collection\n",
    "In order to perform our analysis, we will pull data from 3 different sources.  First will be the raw player data itself. The only source that keeps player level statistics for all CS:GO pro games is hltv.org, a forum devoted to following the game.\n",
    "The second data source we will be looking at is team ranking over time from esl.tv, a website devoted to ranking teams based on tournament wins. Each day the ESL api is updated with team rankings, making it extremely easy to scrape.\n",
    "The last data source we used was from liquipedia.net, a wikipedia like website that provides information on CS:GO players. In CS:GO, players have different roles depending on which kind of weapon they use the most.  Since these roles heavily skew performance, liquipedia was used to determine which role each player had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from time import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scraping HLTV\n",
    "Unfortunately, hltv.org is extremely hard to scrape, as it is gated behind cloudflare and a mix of other security measures designed to combat bots trying to access the forum. In order to access the player data, we use Selenium, which fully emulates a browser. From hltv, we scrape a list of all CS:GO players, as well as performance statistics like kills and deaths for every pro CS:GO game ever played by visiting every player page. This step takes by far the longest, at around 3 hours to run even when multithreaded. For convenience, predownloaded pages are stored in the /scraped pages directory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of hltv players already found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 852/852 [00:00<00:00, 1498.90it/s]\n"
     ]
    }
   ],
   "source": [
    "### A function to download player data given a list of urls\n",
    "def scrape_pages(pages, skip=True):\n",
    "    driver = webdriver.Firefox()\n",
    "    # skip downloading page if it is already found in ./scraped pages\n",
    "    for name in tqdm(pages):\n",
    "        t = time()\n",
    "        url = \"https://www.hltv.org/stats/players/matches/\" + name\n",
    "        if not skip or name.replace(\"/\",\"\") not in os.listdir(\"scraped pages\"):\n",
    "            driver.get(url)\n",
    "            html = driver.page_source\n",
    "            with open(\"scraped pages/{}\".format(name.replace(\"/\",\"\")),\"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html)\n",
    "                f.close()\n",
    "            print(\"Scraped {}, took {} seconds\".format(name,time()-t))\n",
    "    driver.close()\n",
    "\n",
    "# Download a list of pro CS:GO players from hltv\n",
    "def scrape_names():\n",
    "    # download player list if not already a local file\n",
    "    if \"players_page.html\" not in os.listdir(\".\"):\n",
    "        driver = webdriver.Firefox()\n",
    "        url = \"https://www.hltv.org/stats/players/\"\n",
    "        driver.get(url)\n",
    "        print(\"Scraping list of players\")\n",
    "        html = driver.page_source\n",
    "        with open(\"players_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html)\n",
    "            f.close()\n",
    "        driver.close()\n",
    "    else:\n",
    "        print(\"List of hltv players already found\")\n",
    "\n",
    "# Credit: https://stackoverflow.com/questions/312443/how-do-i-split-a-list-into-equally-sized-chunks\n",
    "# Simple function to split a list into evenly sized chunks\n",
    "def chunks(lst, n):\n",
    "    n = int(len(lst)/n)+1\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "# Function to scrape hltv\n",
    "def scrape_hltv():\n",
    "    n = 1 # number of threads, increasing threads didn't seem to decrease time on my weak computer\n",
    "    scrape_names() # get player names and save\n",
    "    pages = get_player_list(\"players_page.html\") # read in player names\n",
    "    threads = [threading.Thread(target=scrape_pages, args=([subset])) for subset in chunks(pages,n)]\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    print(\"Finished scraping\")\n",
    "\n",
    "# Find a list of players from the hltv html page\n",
    "def get_player_list(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        bs = BeautifulSoup(f, features='lxml')\n",
    "        rows = bs.find('table')\n",
    "        pattern = r\"/stats/players/(.*?)?\\\"\"\n",
    "        return re.findall(pattern, str(rows))\n",
    "\n",
    "scrape_hltv() ### Scrape all of hltv player data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we take all of the scraped html pages and do some light data cleaning to combine them all into one large dataframe, which we then save to a csv for easier use, this takes about 20 minutes to run. Each entry in the resulting dataframe will correspond to a player's performance in a single game. We also precompute some useful stats like kill-to-death ratio which we will later use."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed games already found, skipping\n"
     ]
    }
   ],
   "source": [
    "### Load data into dataframe from scraped html\n",
    "# Skip of the processed csv is already present to save time\n",
    "if \"all games processed.csv\" not in os.listdir(\".\"):\n",
    "    dfs = []\n",
    "    for name in tqdm(os.listdir(\"scraped pages\")):\n",
    "        with open(\"scraped pages/\"+name,'r') as f:\n",
    "            try:\n",
    "                bs = BeautifulSoup(f, features='lxml')\n",
    "                rows = bs.find('table')\n",
    "                df = pd.read_html(str(rows))[0]\n",
    "                df = df.drop(df.columns[-2:], axis=1)\n",
    "                df.columns=[\"Date\", \"Player team\", \"Opponent\", \"Map\", \"kd\", \"pm\", \"rating\"]\n",
    "                df[\"player name\"] = re.sub(\"[0-9]*\",\"\", name) # Remove all numbers from player pages\n",
    "                # This has the side effect of turning names like s1mple into smple\n",
    "\n",
    "                dfs.append(df)\n",
    "            except: # if there is an error in reading, we skip\n",
    "                print(\"Skipping {}\".format(name))\n",
    "            f.close()\n",
    "\n",
    "    full_frame = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    def get_kd(s):\n",
    "        a = s.split(\" - \")\n",
    "        a[1] = 1 if int(a[1]) == 0 else int(a[1])\n",
    "        return int(a[0])/int(a[1])\n",
    "\n",
    "    # We store the kd ratio\n",
    "    full_frame[\"kd ratio\"] = full_frame[\"kd\"].apply(get_kd)\n",
    "\n",
    "    # Extract rounds won from hltv format\n",
    "    full_frame[\"rounds won\"] = full_frame[\"Player team\"].apply(lambda s: int(s[-3:-1].replace(\"(\",\"\")))\n",
    "    # Extract rounds lost\n",
    "    full_frame[\"rounds lost\"] = full_frame[\"Opponent\"].apply(lambda s: int(s[-3:-1].replace(\"(\",\"\")))\n",
    "    # Extract player team\n",
    "    full_frame[\"Player team\"] = full_frame[\"Player team\"].apply(lambda s: s[:-4].strip())\n",
    "    # Extract opponent team\n",
    "    full_frame[\"Opponent\"] = full_frame[\"Opponent\"].apply(lambda s: s[:-4].strip())\n",
    "    # Find total rounds player\n",
    "    full_frame[\"Total rounds\"] = full_frame[\"rounds won\"] + full_frame[\"rounds lost\"]\n",
    "    # Some old ratings have a * to indicate a different system, we strip this\n",
    "    full_frame['rating'] = full_frame['rating'].apply(lambda s: float(s.replace(\"*\", \"\")))\n",
    "    # Find if the player won the game\n",
    "    full_frame['win'] = full_frame['rounds won'] > full_frame['rounds lost']\n",
    "    # Map wins to 1 or -1\n",
    "    full_frame['win'] = full_frame['win'].apply(lambda s: 1 if s else -1)\n",
    "    full_frame.to_csv(\"all games processed.csv\") # Save our csv\n",
    "else:\n",
    "    print(\"Processed games already found, skipping\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scraping team rankings and Imputation\n",
    "Scraping data from pro.eslgaming.com/worldranking/csgo/rankings/ is much easier since they provide an API that responds to date queries with json containing a list of team rankings on that day. In order to download data we just query the days we are interested in.  We store the data in a dictionary that takes in team name and returns another dictionary that takes in a particular date in order to determine rank.  Additionally, some days have missing rankings, so if possible we imputate the missing data with a simple linear regression between two surrounding data points in time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 716/2689 [03:48<11:00,  2.99it/s]"
     ]
    }
   ],
   "source": [
    "### Scrape all team rankings and put it in a big dictionary ###\n",
    "if \"team_rankings.json\" not in os.listdir(\".\"):\n",
    "    url = \"https://cdn1.api.esl.tv/csgo/worldranking/teamhistory/rankings?date=\"\n",
    "    rankings_team_date = {}\n",
    "    # Come up with our date range, starting in 2016 and ending at modern day\n",
    "    dates = pd.date_range(start=\"2015-12-28\", end=\"2023-5-8\")\n",
    "    for date in tqdm(dates):\n",
    "        # make our json query\n",
    "        r = requests.get(url + date.strftime(\"%y-%m-%d\"))\n",
    "        try:\n",
    "            for team in r.json()[\"items\"]:\n",
    "                try: # append this ranking to our team's dictionary at this date\n",
    "                    rankings_team_date[team[\"teaminfo\"][\"name\"]][date.strftime(\"%y-%m-%d\")] = team[\"power_rank\"]\n",
    "                except: # We need to make a new entry for team\n",
    "                    rankings_team_date[team[\"teaminfo\"][\"name\"]] = {}\n",
    "                    rankings_team_date[team[\"teaminfo\"][\"name\"]][date.strftime(\"%y-%m-%d\")] = team[\"power_rank\"]\n",
    "        except: # sometimes a day is not recorded\n",
    "            ...\n",
    "    with open(\"team_rankings.json\",\"w\") as f:\n",
    "        json.dump(rankings_team_date, f)\n",
    "\n",
    "else:\n",
    "    print(\"found team rankings file, reading\")\n",
    "    with open(\"team_rankings.json\",'r') as f:\n",
    "        rankings_team_date = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scraping Role List\n",
    "Here we scrape a list of all players who use the sniper weapon from liquipedia, this weapon is more difficult to obtain and more powerful, so usually there is only one player who uses it in each pro team. This data is needed to account for the corresponding difference in performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Get a list of awpers from liquipedia\n",
    "### Getting awpers\n",
    "urls = [\"https://liquipedia.net/counterstrike/Category:AWPers\",\n",
    "        \"https://liquipedia.net/counterstrike/index.php?title=Category:AWPers&pagefrom=Galantis#mw-pages\",\n",
    "        \"https://liquipedia.net/counterstrike/index.php?title=Category:AWPers&pagefrom=Powell#mw-pages\"]\n",
    "\n",
    "if \"team_rankings.json\" not in os.listdir(\".\"):\n",
    "    awpers = {\"~~~~\"} # Create a lookup table of awpers\n",
    "    for url in urls:\n",
    "        res = requests.get(url)\n",
    "        bs = BeautifulSoup(res.content)\n",
    "        ### Toggle system to only get from first to last player on each page\n",
    "        toggle = False\n",
    "        for l in bs.find_all(\"a\"):\n",
    "            if \"Ace\" in l.get('href') or \"/Galantis\" in l.get('href') or \"/Powell\" in l.get('href'):\n",
    "                toggle=True\n",
    "            if toggle:\n",
    "                # When adding names of awpers to the table, we are extremely loose with what counts\n",
    "                # adding several variations to make sure that when we lookup names from different data\n",
    "                # sources we garuntee a match if we should have one.\n",
    "                s = l.get('href')[15:].lower()\n",
    "                awpers.add(s) # Add base name\n",
    "                if \"_\" in s:\n",
    "                    s = s[:s.find(\"_\")] # add name with extra annotations removed\n",
    "                awpers.add(s)\n",
    "                s = re.sub(r'[^a-zA-Z0-9]', '', s)# Remove extra characters\n",
    "                awpers.add(s)\n",
    "            if \"Gafolo\" in l.get('href') or \"Potei\" in l.get('href') or \"ZywOo\" in l.get('href'):\n",
    "                toggle=False\n",
    "\n",
    "    # Dump data to a json file for future reference\n",
    "    with open(\"awpers.json\", 'w') as file:\n",
    "        json.dump(list(awpers), file)\n",
    "else:\n",
    "    with open('awpers.json') as file:\n",
    "        awpers = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"all games processed.csv\", sep=',')\n",
    "df= df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# cleaning data and labeling awpers\n",
    "df['rating'] = df['rating'].apply(lambda s: float(s.replace(\"*\", \"\")))\n",
    "df['Awper'] = df['player name'].apply(lambda s: s in awpers)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "players = df['player name'].unique()\n",
    "teams = df['Player team'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.assign(Date = pd.to_datetime(df['Date']))\n",
    "df.sort_values('Date').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(2012, 2024)\n",
    "y = []\n",
    "y_awp = []\n",
    "y_rifle = []\n",
    "\n",
    "# determining mean performance per year and standardizing rating\n",
    "for year in x:\n",
    "    date_mask = (dt.datetime(year,1,1) < df['Date']) & (df['Date'] < dt.datetime(year,12,31))\n",
    "    y.append(df[date_mask]['rating'].mean())\n",
    "    mean_awp = df[date_mask & df['Awper']]['rating'].mean()\n",
    "    y_awp.append(mean_awp)\n",
    "    mean_rifle = df[date_mask & ~df['Awper']]['rating'].mean()\n",
    "    y_rifle.append(mean_rifle)\n",
    "\n",
    "    std_awp = df[date_mask & df['Awper']]['rating'].std()\n",
    "    std_rifle = df[date_mask & ~df['Awper']]['rating'].std()\n",
    "    \n",
    "    df.loc[date_mask & df['Awper'],'standard rating'] = (df['rating']-mean_awp) / std_awp\n",
    "    df.loc[date_mask & ~df['Awper'],'standard rating'] = (df['rating']-mean_rifle) / std_rifle\n",
    "\n",
    "plt.plot(x,y,label = 'all players')\n",
    "plt.plot(x,y_awp,label = 'Awpers')\n",
    "plt.plot(x,y_rifle,label = 'Riflers')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Rating\")\n",
    "plt.title(\"Average yearly rating\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}